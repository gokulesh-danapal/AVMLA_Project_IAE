{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"YoloV3_Kitti.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"MHDsdPszmU5O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"status":"ok","timestamp":1594735160133,"user_tz":-330,"elapsed":36775,"user":{"displayName":"Gokulesh Danapal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCM1dcJPLvB_sAE4Xcgy-SRhc1CMcCDdQmWfVTIU8=s64","userId":"07339810366620379654"}},"outputId":"d504f522-f6f8-4719-8ccc-d5e3aa38e533"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd /content/gdrive/My Drive/Kitti_Image/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n","/content/gdrive/My Drive/Kitti_Image\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k3njPwXOl_zS","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","\n","from preprocess import Preprocessor\n","from yolov3 import YoloV3\n","from postprocess import Postprocessor\n","\n","num_classes = 3\n","class_names= ['Car','Cyclist','Pedestrian']\n","dest_path = \"/content/gdrive/My Drive/Kitti_Image/input/detection-results\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0ydCmeDsPNcr","colab_type":"text"},"source":["**Train YOLO**"]},{"cell_type":"code","metadata":{"id":"PhuHGWriPdo1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"c631a930-c99c-4c48-946d-22e160d9dfc7"},"source":["!python train.py\n","#python train.py --checkpoint '/content/gdrive/My Drive/kitti/models/model-v1.0.1-epoch-10-loss-0.0065.tf'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Trained batch: 11 batch loss: 66.8591232 batch xy loss 10.3867378 batch wh loss 10.9088926 batch obj loss 41.1126518 batch_class_loss 4.45083714 epoch total loss: 1167.50769\n","Trained batch: 12 batch loss: 54.3849182 batch xy loss 8.64699459 batch wh loss 7.77132797 batch obj loss 34.9787102 batch_class_loss 2.98788333 epoch total loss: 1221.89258\n","Trained batch: 13 batch loss: 151.679108 batch xy loss 9.19206238 batch wh loss 104.791023 batch obj loss 33.6468811 batch_class_loss 4.04912281 epoch total loss: 1373.57166\n","Trained batch: 14 batch loss: 61.9763641 batch xy loss 9.14313412 batch wh loss 16.0870476 batch obj loss 30.4811859 batch_class_loss 6.26499271 epoch total loss: 1435.54797\n","Trained batch: 15 batch loss: 57.6565399 batch xy loss 9.05611897 batch wh loss 11.3672 batch obj loss 33.9155464 batch_class_loss 3.31767368 epoch total loss: 1493.20447\n","Trained batch: 16 batch loss: 69.4134445 batch xy loss 8.5512743 batch wh loss 19.2202339 batch obj loss 32.3824 batch_class_loss 9.25953865 epoch total loss: 1562.61792\n","Trained batch: 17 batch loss: 33.9708328 batch xy loss 4.70086432 batch wh loss 6.70222282 batch obj loss 20.1883602 batch_class_loss 2.37938738 epoch total loss: 1596.58875\n","Trained batch: 18 batch loss: 39.5379715 batch xy loss 5.47535658 batch wh loss 7.41097879 batch obj loss 22.3854294 batch_class_loss 4.26620436 epoch total loss: 1636.12671\n","Trained batch: 19 batch loss: 38.584549 batch xy loss 5.12419224 batch wh loss 9.1794157 batch obj loss 20.6631489 batch_class_loss 3.61779594 epoch total loss: 1674.7113\n","Trained batch: 20 batch loss: 40.8825073 batch xy loss 6.54768133 batch wh loss 7.86859465 batch obj loss 23.8754597 batch_class_loss 2.59077573 epoch total loss: 1715.59375\n","Trained batch: 21 batch loss: 39.4235764 batch xy loss 6.31167507 batch wh loss 7.19540691 batch obj loss 23.4769287 batch_class_loss 2.43957067 epoch total loss: 1755.01733\n","Trained batch: 22 batch loss: 50.0295563 batch xy loss 7.16137362 batch wh loss 10.6395054 batch obj loss 28.1343689 batch_class_loss 4.09430408 epoch total loss: 1805.04688\n","Trained batch: 23 batch loss: 66.6664581 batch xy loss 8.78116 batch wh loss 15.9428787 batch obj loss 35.0619507 batch_class_loss 6.88047504 epoch total loss: 1871.71338\n","Trained batch: 24 batch loss: 58.8880768 batch xy loss 9.86633 batch wh loss 10.9506731 batch obj loss 34.5229607 batch_class_loss 3.54811573 epoch total loss: 1930.60144\n","Trained batch: 25 batch loss: 59.5845261 batch xy loss 7.90917063 batch wh loss 14.1982574 batch obj loss 33.2576981 batch_class_loss 4.2193985 epoch total loss: 1990.18591\n","Trained batch: 26 batch loss: 46.1758881 batch xy loss 6.90877628 batch wh loss 8.94139099 batch obj loss 26.1203117 batch_class_loss 4.20541 epoch total loss: 2036.36182\n","Trained batch: 27 batch loss: 54.5498619 batch xy loss 7.36440134 batch wh loss 10.9772091 batch obj loss 31.320694 batch_class_loss 4.88755846 epoch total loss: 2090.91162\n","Trained batch: 28 batch loss: 44.9166145 batch xy loss 7.43381596 batch wh loss 7.08889341 batch obj loss 27.7168922 batch_class_loss 2.67701125 epoch total loss: 2135.82812\n","Trained batch: 29 batch loss: 41.7136497 batch xy loss 6.76290941 batch wh loss 5.92263174 batch obj loss 26.9263687 batch_class_loss 2.10174108 epoch total loss: 2177.54175\n","Trained batch: 30 batch loss: 56.5956879 batch xy loss 6.90630817 batch wh loss 13.5663824 batch obj loss 30.0917 batch_class_loss 6.0312953 epoch total loss: 2234.13745\n","Trained batch: 31 batch loss: 48.0161324 batch xy loss 7.67416191 batch wh loss 7.9559536 batch obj loss 29.0210075 batch_class_loss 3.36500454 epoch total loss: 2282.15356\n","Trained batch: 32 batch loss: 45.5964928 batch xy loss 7.92441702 batch wh loss 7.10833406 batch obj loss 28.309515 batch_class_loss 2.25422668 epoch total loss: 2327.75\n","Trained batch: 33 batch loss: 31.755167 batch xy loss 4.53009033 batch wh loss 5.10690355 batch obj loss 20.5124207 batch_class_loss 1.60575449 epoch total loss: 2359.50513\n","Trained batch: 34 batch loss: 547.125732 batch xy loss 6.36462593 batch wh loss 509.407837 batch obj loss 24.2991581 batch_class_loss 7.05404902 epoch total loss: 2906.63086\n","Trained batch: 35 batch loss: 53.770546 batch xy loss 8.04066563 batch wh loss 11.0284033 batch obj loss 29.2469749 batch_class_loss 5.45450354 epoch total loss: 2960.40137\n","Trained batch: 36 batch loss: 53.6504402 batch xy loss 8.14776 batch wh loss 9.73730755 batch obj loss 31.7325363 batch_class_loss 4.03284 epoch total loss: 3014.05176\n","Trained batch: 37 batch loss: 38.2971573 batch xy loss 6.61758947 batch wh loss 5.75792694 batch obj loss 24.2434349 batch_class_loss 1.67820418 epoch total loss: 3052.34888\n","Trained batch: 38 batch loss: 47.1211777 batch xy loss 6.95439 batch wh loss 9.41075706 batch obj loss 26.1801548 batch_class_loss 4.5758791 epoch total loss: 3099.47\n","Trained batch: 39 batch loss: 57.8019524 batch xy loss 6.28578949 batch wh loss 15.8833199 batch obj loss 29.8188934 batch_class_loss 5.81394863 epoch total loss: 3157.27197\n","Trained batch: 40 batch loss: 55.3739548 batch xy loss 8.6608305 batch wh loss 9.76686 batch obj loss 33.5331078 batch_class_loss 3.41316032 epoch total loss: 3212.646\n","Trained batch: 41 batch loss: 49.1787758 batch xy loss 6.74006319 batch wh loss 11.1173763 batch obj loss 27.1812096 batch_class_loss 4.14012527 epoch total loss: 3261.82471\n","Trained batch: 42 batch loss: 92.2662735 batch xy loss 7.39091206 batch wh loss 51.4265823 batch obj loss 29.9839478 batch_class_loss 3.46483636 epoch total loss: 3354.09106\n","Trained batch: 43 batch loss: 55.1940689 batch xy loss 9.09658623 batch wh loss 10.6106558 batch obj loss 32.2862854 batch_class_loss 3.20054054 epoch total loss: 3409.28516\n","Trained batch: 44 batch loss: 55.3072166 batch xy loss 8.32087898 batch wh loss 12.8577356 batch obj loss 29.6199265 batch_class_loss 4.50867081 epoch total loss: 3464.59229\n","Trained batch: 45 batch loss: 51.0340805 batch xy loss 7.77545357 batch wh loss 10.3129301 batch obj loss 28.2434082 batch_class_loss 4.70228529 epoch total loss: 3515.62646\n","Trained batch: 46 batch loss: 55.7208138 batch xy loss 7.91279602 batch wh loss 13.8672504 batch obj loss 27.6547832 batch_class_loss 6.2859807 epoch total loss: 3571.34717\n","Trained batch: 47 batch loss: 185.599304 batch xy loss 9.57414246 batch wh loss 139.698868 batch obj loss 33.8756447 batch_class_loss 2.45061755 epoch total loss: 3756.94653\n","Trained batch: 48 batch loss: 50.3074951 batch xy loss 8.1264019 batch wh loss 9.50603485 batch obj loss 29.1576519 batch_class_loss 3.51740766 epoch total loss: 3807.25391\n","Trained batch: 49 batch loss: 59.1863136 batch xy loss 6.53031492 batch wh loss 16.9032764 batch obj loss 27.2772865 batch_class_loss 8.47543621 epoch total loss: 3866.44019\n","Trained batch: 50 batch loss: 73.171936 batch xy loss 9.61061764 batch wh loss 16.5527878 batch obj loss 37.6863861 batch_class_loss 9.32214069 epoch total loss: 3939.61206\n","Trained batch: 51 batch loss: 67.3985901 batch xy loss 8.44728565 batch wh loss 15.7302904 batch obj loss 33.7593193 batch_class_loss 9.46169376 epoch total loss: 4007.01074\n","Trained batch: 52 batch loss: 50.7900085 batch xy loss 8.25507164 batch wh loss 10.2087049 batch obj loss 27.9959679 batch_class_loss 4.33026457 epoch total loss: 4057.80078\n","Trained batch: 53 batch loss: 43.8676605 batch xy loss 6.355124 batch wh loss 9.34246445 batch obj loss 23.7922668 batch_class_loss 4.37780333 epoch total loss: 4101.66846\n","Trained batch: 54 batch loss: 50.2862892 batch xy loss 7.72171926 batch wh loss 11.0550156 batch obj loss 28.3363571 batch_class_loss 3.17319894 epoch total loss: 4151.95459\n","Trained batch: 55 batch loss: 46.4199295 batch xy loss 6.21933317 batch wh loss 11.0285826 batch obj loss 24.1440601 batch_class_loss 5.0279541 epoch total loss: 4198.37451\n","Trained batch: 56 batch loss: 60.9802246 batch xy loss 8.59209156 batch wh loss 13.8358936 batch obj loss 31.8014469 batch_class_loss 6.75079107 epoch total loss: 4259.35449\n","Trained batch: 57 batch loss: 53.3656578 batch xy loss 7.54579 batch wh loss 12.0635977 batch obj loss 29.289011 batch_class_loss 4.46725941 epoch total loss: 4312.72\n","Trained batch: 58 batch loss: 61.4271088 batch xy loss 8.6457 batch wh loss 15.0446501 batch obj loss 33.232151 batch_class_loss 4.5046072 epoch total loss: 4374.14746\n","Trained batch: 59 batch loss: 39.4367828 batch xy loss 4.22191715 batch wh loss 8.49496555 batch obj loss 22.3215847 batch_class_loss 4.39831686 epoch total loss: 4413.58447\n","Trained batch: 60 batch loss: 57.1990204 batch xy loss 8.55094433 batch wh loss 13.942214 batch obj loss 29.4022083 batch_class_loss 5.30365753 epoch total loss: 4470.78369\n","Trained batch: 61 batch loss: 38.321 batch xy loss 4.44993734 batch wh loss 8.04445171 batch obj loss 21.6102886 batch_class_loss 4.21631956 epoch total loss: 4509.10449\n","Trained batch: 62 batch loss: 1052.19421 batch xy loss 6.40901232 batch wh loss 1015.82623 batch obj loss 26.2999878 batch_class_loss 3.65893626 epoch total loss: 5561.29883\n","Trained batch: 63 batch loss: 51.5686111 batch xy loss 8.92019 batch wh loss 9.13691711 batch obj loss 30.3164253 batch_class_loss 3.19508195 epoch total loss: 5612.86768\n","Trained batch: 64 batch loss: 44.5437088 batch xy loss 6.21443462 batch wh loss 8.68558216 batch obj loss 25.8423195 batch_class_loss 3.801373 epoch total loss: 5657.41162\n","Trained batch: 65 batch loss: 59.1364365 batch xy loss 9.16796303 batch wh loss 10.5601149 batch obj loss 34.7316933 batch_class_loss 4.67666245 epoch total loss: 5716.54785\n","Trained batch: 66 batch loss: 58.1734276 batch xy loss 8.38020802 batch wh loss 12.0512753 batch obj loss 33.3342896 batch_class_loss 4.40765619 epoch total loss: 5774.72119\n","Trained batch: 67 batch loss: 52.1025314 batch xy loss 6.33155155 batch wh loss 11.0210905 batch obj loss 29.718111 batch_class_loss 5.0317812 epoch total loss: 5826.82373\n","Trained batch: 68 batch loss: 62.0959778 batch xy loss 5.95749617 batch wh loss 20.0324879 batch obj loss 29.5702419 batch_class_loss 6.53575 epoch total loss: 5888.92\n","Trained batch: 69 batch loss: 39.3072128 batch xy loss 5.79480314 batch wh loss 6.80230141 batch obj loss 24.555275 batch_class_loss 2.15483618 epoch total loss: 5928.22705\n","Trained batch: 70 batch loss: 41.7358093 batch xy loss 7.86410666 batch wh loss 6.01628447 batch obj loss 25.1884689 batch_class_loss 2.66694355 epoch total loss: 5969.96289\n","Trained batch: 71 batch loss: 64.4607391 batch xy loss 11.152441 batch wh loss 14.5122061 batch obj loss 32.4144287 batch_class_loss 6.38166332 epoch total loss: 6034.42383\n","Trained batch: 72 batch loss: 48.0696602 batch xy loss 7.92462921 batch wh loss 6.17862463 batch obj loss 31.8555126 batch_class_loss 2.11089754 epoch total loss: 6082.49365\n","Trained batch: 73 batch loss: 63.2563515 batch xy loss 8.22479343 batch wh loss 16.5029793 batch obj loss 32.4108124 batch_class_loss 6.11776543 epoch total loss: 6145.75\n","Trained batch: 74 batch loss: 53.4199371 batch xy loss 8.0918293 batch wh loss 9.75562859 batch obj loss 32.4969864 batch_class_loss 3.07549167 epoch total loss: 6199.17\n","Trained batch: 75 batch loss: 54.5846 batch xy loss 6.90807247 batch wh loss 13.075016 batch obj loss 29.3844872 batch_class_loss 5.2170229 epoch total loss: 6253.75439\n","Trained batch: 76 batch loss: 60.390892 batch xy loss 7.81097746 batch wh loss 14.7552643 batch obj loss 29.6849575 batch_class_loss 8.13969326 epoch total loss: 6314.14551\n","Trained batch: 77 batch loss: 57.6764565 batch xy loss 8.84592533 batch wh loss 11.5236206 batch obj loss 33.9881516 batch_class_loss 3.3187623 epoch total loss: 6371.82178\n","Trained batch: 78 batch loss: 57.8821945 batch xy loss 8.71226692 batch wh loss 11.9789295 batch obj loss 30.8331394 batch_class_loss 6.35785723 epoch total loss: 6429.7041\n","Trained batch: 79 batch loss: 53.6905327 batch xy loss 7.90124083 batch wh loss 9.71400356 batch obj loss 31.5376282 batch_class_loss 4.53765917 epoch total loss: 6483.39453\n","Trained batch: 80 batch loss: 46.8413467 batch xy loss 6.81565475 batch wh loss 9.66142559 batch obj loss 25.8228321 batch_class_loss 4.54143524 epoch total loss: 6530.23584\n","Trained batch: 81 batch loss: 41.9950638 batch xy loss 6.36349201 batch wh loss 7.8678894 batch obj loss 24.433712 batch_class_loss 3.3299706 epoch total loss: 6572.23096\n","Trained batch: 82 batch loss: 64.7233582 batch xy loss 9.39421654 batch wh loss 14.3160219 batch obj loss 35.1883736 batch_class_loss 5.82475 epoch total loss: 6636.9541\n","Trained batch: 83 batch loss: 63.771656 batch xy loss 10.0719767 batch wh loss 13.480669 batch obj loss 33.5611305 batch_class_loss 6.65788174 epoch total loss: 6700.72559\n","Trained batch: 84 batch loss: 39.7192879 batch xy loss 5.56399965 batch wh loss 7.37819958 batch obj loss 24.5386181 batch_class_loss 2.23847342 epoch total loss: 6740.44482\n","Trained batch: 85 batch loss: 46.6230965 batch xy loss 7.01851606 batch wh loss 8.06591892 batch obj loss 28.1983776 batch_class_loss 3.34028268 epoch total loss: 6787.06787\n","Trained batch: 86 batch loss: 53.1118622 batch xy loss 6.9260025 batch wh loss 12.6724606 batch obj loss 27.5453606 batch_class_loss 5.96803808 epoch total loss: 6840.17969\n","Trained batch: 87 batch loss: 2990.60205 batch xy loss 7.81404114 batch wh loss 2948.31323 batch obj loss 30.7436485 batch_class_loss 3.73152733 epoch total loss: 9830.78125\n","Trained batch: 88 batch loss: 46.8531876 batch xy loss 6.24908304 batch wh loss 9.94965458 batch obj loss 26.8332367 batch_class_loss 3.82120728 epoch total loss: 9877.63477\n","Trained batch: 89 batch loss: 56.6133919 batch xy loss 7.41326714 batch wh loss 11.462574 batch obj loss 33.0940361 batch_class_loss 4.64351082 epoch total loss: 9934.24805\n","Trained batch: 90 batch loss: 58.7492676 batch xy loss 7.92375469 batch wh loss 12.3214674 batch obj loss 34.4677124 batch_class_loss 4.03633118 epoch total loss: 9992.99707\n","Trained batch: 91 batch loss: 55.9727325 batch xy loss 6.98767138 batch wh loss 13.6707659 batch obj loss 31.5964432 batch_class_loss 3.71784925 epoch total loss: 10048.9697\n","Trained batch: 92 batch loss: 1517.19983 batch xy loss 8.4913826 batch wh loss 1472.22778 batch obj loss 32.7078323 batch_class_loss 3.7728343 epoch total loss: 11566.1699\n","Trained batch: 93 batch loss: 849.66217 batch xy loss 9.27173328 batch wh loss 798.39624 batch obj loss 37.8273087 batch_class_loss 4.16689682 epoch total loss: 12415.832\n","Trained batch: 94 batch loss: 62.1077499 batch xy loss 7.20928907 batch wh loss 24.4363689 batch obj loss 26.6102352 batch_class_loss 3.85185981 epoch total loss: 12477.9395\n","Trained batch: 95 batch loss: 347.592468 batch xy loss 7.04781628 batch wh loss 305.868195 batch obj loss 29.7693539 batch_class_loss 4.90712452 epoch total loss: 12825.5322\n","Trained batch: 96 batch loss: 192.635132 batch xy loss 7.88502645 batch wh loss 145.612854 batch obj loss 35.9359169 batch_class_loss 3.20134 epoch total loss: 13018.167\n","Trained batch: 97 batch loss: 72.2020187 batch xy loss 6.44817162 batch wh loss 35.7708397 batch obj loss 25.9849796 batch_class_loss 3.99803567 epoch total loss: 13090.3691\n","Trained batch: 98 batch loss: 69.4555206 batch xy loss 6.21641731 batch wh loss 32.7915955 batch obj loss 27.5143871 batch_class_loss 2.93311596 epoch total loss: 13159.8242\n","Trained batch: 99 batch loss: 79.8815613 batch xy loss 9.20383072 batch wh loss 32.5262032 batch obj loss 35.6341629 batch_class_loss 2.51736569 epoch total loss: 13239.7061\n","Trained batch: 100 batch loss: 56.8613167 batch xy loss 7.99960041 batch wh loss 14.8992863 batch obj loss 29.668314 batch_class_loss 4.29411364 epoch total loss: 13296.5674\n","Trained batch: 101 batch loss: 89.9994278 batch xy loss 10.0414495 batch wh loss 28.7689114 batch obj loss 41.5441742 batch_class_loss 9.64489174 epoch total loss: 13386.5664\n","Trained batch: 102 batch loss: 67.0856171 batch xy loss 6.63716793 batch wh loss 23.4117203 batch obj loss 31.1403408 batch_class_loss 5.8963933 epoch total loss: 13453.6523\n","Trained batch: 103 batch loss: 71.3240585 batch xy loss 10.1020794 batch wh loss 20.4930687 batch obj loss 37.0399094 batch_class_loss 3.68900251 epoch total loss: 13524.9766\n","Trained batch: 104 batch loss: 89.7338715 batch xy loss 11.0593338 batch wh loss 32.0985184 batch obj loss 39.1945419 batch_class_loss 7.38146925 epoch total loss: 13614.71\n","Trained batch: 105 batch loss: 246.121765 batch xy loss 6.81920862 batch wh loss 204.606171 batch obj loss 31.1918964 batch_class_loss 3.50449109 epoch total loss: 13860.832\n","Trained batch: 106 batch loss: 97.1173935 batch xy loss 10.9433727 batch wh loss 36.145195 batch obj loss 43.1881065 batch_class_loss 6.84070969 epoch total loss: 13957.9492\n","Trained batch: 107 batch loss: 68.9349365 batch xy loss 7.76730824 batch wh loss 20.7530212 batch obj loss 36.8393707 batch_class_loss 3.57523918 epoch total loss: 14026.8838\n","Trained batch: 108 batch loss: 73.3182373 batch xy loss 8.73722 batch wh loss 17.3715401 batch obj loss 38.7113 batch_class_loss 8.49817562 epoch total loss: 14100.2021\n","Trained batch: 109 batch loss: 68.4510498 batch xy loss 10.3395882 batch wh loss 13.2766294 batch obj loss 41.1072693 batch_class_loss 3.72756553 epoch total loss: 14168.6533\n","Trained batch: 110 batch loss: 56.3349953 batch xy loss 7.70099545 batch wh loss 9.78309 batch obj loss 35.1416779 batch_class_loss 3.70923114 epoch total loss: 14224.9883\n","Trained batch: 111 batch loss: 37.2653694 batch xy loss 5.10134411 batch wh loss 4.96157646 batch obj loss 24.7918644 batch_class_loss 2.41058373 epoch total loss: 14262.2539\n","Trained batch: 112 batch loss: 51.0076523 batch xy loss 6.3979373 batch wh loss 10.313839 batch obj loss 28.6174145 batch_class_loss 5.67846298 epoch total loss: 14313.2617\n","Trained batch: 113 batch loss: 79.3569794 batch xy loss 9.47231865 batch wh loss 19.7239399 batch obj loss 39.7306023 batch_class_loss 10.4301128 epoch total loss: 14392.6191\n","Trained batch: 114 batch loss: 137.787979 batch xy loss 9.94855118 batch wh loss 85.3932114 batch obj loss 39.2169189 batch_class_loss 3.22930956 epoch total loss: 14530.4072\n","Trained batch: 115 batch loss: 88.4763718 batch xy loss 7.01731968 batch wh loss 45.9264259 batch obj loss 31.9008102 batch_class_loss 3.63181663 epoch total loss: 14618.8838\n","Trained batch: 116 batch loss: 49.4100113 batch xy loss 6.26535559 batch wh loss 10.2014132 batch obj loss 28.7104855 batch_class_loss 4.23275661 epoch total loss: 14668.2939\n","Trained batch: 117 batch loss: 68.5888824 batch xy loss 10.5616016 batch wh loss 14.5691757 batch obj loss 39.9932556 batch_class_loss 3.464849 epoch total loss: 14736.8828\n","Trained batch: 118 batch loss: 66.4391251 batch xy loss 9.2357626 batch wh loss 13.175828 batch obj loss 39.0691833 batch_class_loss 4.95834732 epoch total loss: 14803.3223\n","Trained batch: 119 batch loss: 51.8931274 batch xy loss 7.40458345 batch wh loss 10.0298815 batch obj loss 31.7931404 batch_class_loss 2.66552329 epoch total loss: 14855.2158\n","Trained batch: 120 batch loss: 44.9207726 batch xy loss 6.52895594 batch wh loss 5.69909286 batch obj loss 31.1835613 batch_class_loss 1.50916421 epoch total loss: 14900.1367\n","Trained batch: 121 batch loss: 42.4246902 batch xy loss 6.5807128 batch wh loss 5.83326054 batch obj loss 27.428339 batch_class_loss 2.58237743 epoch total loss: 14942.5615\n","Trained batch: 122 batch loss: 61.4469833 batch xy loss 7.81315517 batch wh loss 16.493103 batch obj loss 31.292942 batch_class_loss 5.84779024 epoch total loss: 15004.0088\n","Trained batch: 123 batch loss: 63.0606079 batch xy loss 6.95116568 batch wh loss 13.8828917 batch obj loss 35.884491 batch_class_loss 6.34206 epoch total loss: 15067.0693\n","Trained batch: 124 batch loss: 45.5726852 batch xy loss 6.39996 batch wh loss 7.76032972 batch obj loss 28.6442776 batch_class_loss 2.76811624 epoch total loss: 15112.6416\n","Trained batch: 125 batch loss: 49.8528671 batch xy loss 6.15710211 batch wh loss 16.4377918 batch obj loss 25.3787308 batch_class_loss 1.87924826 epoch total loss: 15162.4941\n","Trained batch: 126 batch loss: 62.2278824 batch xy loss 7.5634985 batch wh loss 15.3624372 batch obj loss 32.7416496 batch_class_loss 6.56029844 epoch total loss: 15224.7217\n","Trained batch: 127 batch loss: 70.8027725 batch xy loss 11.0108461 batch wh loss 13.4999466 batch obj loss 42.2459869 batch_class_loss 4.04599714 epoch total loss: 15295.5244\n","Trained batch: 128 batch loss: 70.4136887 batch xy loss 8.82298183 batch wh loss 17.5860844 batch obj loss 34.9364281 batch_class_loss 9.06819344 epoch total loss: 15365.9385\n","Trained batch: 129 batch loss: 65.0929 batch xy loss 8.1166687 batch wh loss 15.3673677 batch obj loss 35.4516678 batch_class_loss 6.15719795 epoch total loss: 15431.0312\n","Trained batch: 130 batch loss: 57.4128 batch xy loss 7.94390869 batch wh loss 10.4712248 batch obj loss 35.1461678 batch_class_loss 3.85149646 epoch total loss: 15488.4443\n","Trained batch: 131 batch loss: 55.3774948 batch xy loss 8.31586647 batch wh loss 10.6779957 batch obj loss 32.1248741 batch_class_loss 4.25876 epoch total loss: 15543.8223\n","Trained batch: 132 batch loss: 84.7503891 batch xy loss 8.64353752 batch wh loss 29.434927 batch obj loss 36.6978416 batch_class_loss 9.97408581 epoch total loss: 15628.5723\n","Trained batch: 133 batch loss: 65.2075 batch xy loss 8.19701099 batch wh loss 15.4247 batch obj loss 36.0138321 batch_class_loss 5.57194948 epoch total loss: 15693.7793\n","Trained batch: 134 batch loss: 59.6211395 batch xy loss 7.9936409 batch wh loss 12.9845915 batch obj loss 33.3181839 batch_class_loss 5.32472467 epoch total loss: 15753.4\n","Trained batch: 135 batch loss: 53.730793 batch xy loss 8.14148808 batch wh loss 10.8873644 batch obj loss 30.8303642 batch_class_loss 3.87157345 epoch total loss: 15807.1309\n","Trained batch: 136 batch loss: 50.3800735 batch xy loss 7.91196966 batch wh loss 11.1815262 batch obj loss 28.064436 batch_class_loss 3.22214246 epoch total loss: 15857.5107\n","Trained batch: 137 batch loss: 44.5582619 batch xy loss 5.76492 batch wh loss 11.1039982 batch obj loss 23.9231491 batch_class_loss 3.76619411 epoch total loss: 15902.0693\n","Trained batch: 138 batch loss: 67.2858429 batch xy loss 10.1158457 batch wh loss 14.4372854 batch obj loss 36.7687836 batch_class_loss 5.96392441 epoch total loss: 15969.3555\n","Trained batch: 139 batch loss: 63.9961319 batch xy loss 9.33397579 batch wh loss 13.1512604 batch obj loss 36.6386452 batch_class_loss 4.8722558 epoch total loss: 16033.3516\n","Trained batch: 140 batch loss: 51.1129265 batch xy loss 6.5622716 batch wh loss 10.5589504 batch obj loss 30.2147884 batch_class_loss 3.77691603 epoch total loss: 16084.4648\n","Trained batch: 141 batch loss: 69.7530365 batch xy loss 8.8783474 batch wh loss 18.6277466 batch obj loss 34.6731453 batch_class_loss 7.57379532 epoch total loss: 16154.2178\n","Trained batch: 142 batch loss: 46.8875923 batch xy loss 6.68882179 batch wh loss 7.62880707 batch obj loss 30.4834404 batch_class_loss 2.08652186 epoch total loss: 16201.1055\n","Trained batch: 143 batch loss: 47.9200745 batch xy loss 6.52769279 batch wh loss 9.03342056 batch obj loss 29.6114883 batch_class_loss 2.74747109 epoch total loss: 16249.0254\n","Trained batch: 144 batch loss: 52.6884842 batch xy loss 8.12000561 batch wh loss 9.87135315 batch obj loss 28.7950344 batch_class_loss 5.90209198 epoch total loss: 16301.7139\n","Trained batch: 145 batch loss: 44.6072235 batch xy loss 6.76519299 batch wh loss 6.74504 batch obj loss 27.3764763 batch_class_loss 3.72051644 epoch total loss: 16346.3213\n","Trained batch: 146 batch loss: 63.1840401 batch xy loss 8.40520477 batch wh loss 16.4873066 batch obj loss 33.0860558 batch_class_loss 5.20546913 epoch total loss: 16409.5059\n","Trained batch: 147 batch loss: 49.9596939 batch xy loss 6.87012053 batch wh loss 9.68466 batch obj loss 29.9637146 batch_class_loss 3.44120026 epoch total loss: 16459.4648\n","Trained batch: 148 batch loss: 53.39534 batch xy loss 6.67331 batch wh loss 12.5129213 batch obj loss 28.5837669 batch_class_loss 5.62533855 epoch total loss: 16512.8594\n","Trained batch: 149 batch loss: 50.1085129 batch xy loss 6.77688599 batch wh loss 11.0680408 batch obj loss 28.7796364 batch_class_loss 3.48394537 epoch total loss: 16562.9688\n","Trained batch: 150 batch loss: 39.7309647 batch xy loss 4.78014183 batch wh loss 7.83895874 batch obj loss 23.0866451 batch_class_loss 4.02521801 epoch total loss: 16602.7\n","Trained batch: 151 batch loss: 54.8722076 batch xy loss 8.48748302 batch wh loss 10.1168175 batch obj loss 32.0400162 batch_class_loss 4.22789478 epoch total loss: 16657.5723\n","Trained batch: 152 batch loss: 40.2876892 batch xy loss 5.81836033 batch wh loss 7.30572271 batch obj loss 23.9628906 batch_class_loss 3.20071363 epoch total loss: 16697.8594\n","Trained batch: 153 batch loss: 48.8690147 batch xy loss 7.27154779 batch wh loss 10.0960398 batch obj loss 27.6851273 batch_class_loss 3.81630135 epoch total loss: 16746.7285\n","Trained batch: 154 batch loss: 47.0101891 batch xy loss 6.28422737 batch wh loss 7.95715666 batch obj loss 29.245594 batch_class_loss 3.52321053 epoch total loss: 16793.7383\n","Trained batch: 155 batch loss: 80.8708878 batch xy loss 11.1058302 batch wh loss 16.4444752 batch obj loss 45.3653336 batch_class_loss 7.95523787 epoch total loss: 16874.6094\n","Trained batch: 156 batch loss: 70.6884689 batch xy loss 11.2565346 batch wh loss 15.4870758 batch obj loss 39.3564796 batch_class_loss 4.58837652 epoch total loss: 16945.2969\n","Trained batch: 157 batch loss: 47.4845543 batch xy loss 5.08944607 batch wh loss 15.9105577 batch obj loss 22.275013 batch_class_loss 4.20953655 epoch total loss: 16992.7812\n","Trained batch: 158 batch loss: 49.0836449 batch xy loss 7.14086151 batch wh loss 9.54193401 batch obj loss 29.1202259 batch_class_loss 3.28062105 epoch total loss: 17041.8652\n","Trained batch: 159 batch loss: 38.6769257 batch xy loss 5.63417292 batch wh loss 7.21299362 batch obj loss 23.0742931 batch_class_loss 2.75546813 epoch total loss: 17080.543\n","Trained batch: 160 batch loss: 50.658165 batch xy loss 5.74465084 batch wh loss 11.5754089 batch obj loss 26.2983036 batch_class_loss 7.0398016 epoch total loss: 17131.2012\n","Trained batch: 161 batch loss: 41.0559082 batch xy loss 5.45876789 batch wh loss 8.23241234 batch obj loss 23.9192085 batch_class_loss 3.44552135 epoch total loss: 17172.2578\n","Trained batch: 162 batch loss: 52.9993896 batch xy loss 7.70244408 batch wh loss 9.28871346 batch obj loss 32.0705338 batch_class_loss 3.93770266 epoch total loss: 17225.2578\n","Trained batch: 163 batch loss: 50.9098396 batch xy loss 8.14689636 batch wh loss 9.46012306 batch obj loss 31.2330894 batch_class_loss 2.06972766 epoch total loss: 17276.168\n","Trained batch: 164 batch loss: 44.4881668 batch xy loss 6.01708126 batch wh loss 9.03775501 batch obj loss 26.667223 batch_class_loss 2.76610923 epoch total loss: 17320.6562\n","Trained batch: 165 batch loss: 69.047493 batch xy loss 8.79737377 batch wh loss 22.2445335 batch obj loss 33.0546227 batch_class_loss 4.95096397 epoch total loss: 17389.7031\n","Trained batch: 166 batch loss: 65.5210724 batch xy loss 7.80310917 batch wh loss 16.6771584 batch obj loss 33.1930084 batch_class_loss 7.84779739 epoch total loss: 17455.2246\n","Trained batch: 167 batch loss: 49.8412399 batch xy loss 7.34088516 batch wh loss 9.52372646 batch obj loss 29.2908936 batch_class_loss 3.6857388 epoch total loss: 17505.0664\n","Trained batch: 168 batch loss: 65.6023407 batch xy loss 7.94986534 batch wh loss 27.1752396 batch obj loss 28.0176392 batch_class_loss 2.45960379 epoch total loss: 17570.668\n","Trained batch: 169 batch loss: 58.9383583 batch xy loss 8.06569672 batch wh loss 15.9543695 batch obj loss 29.9272728 batch_class_loss 4.99102068 epoch total loss: 17629.6055\n","Trained batch: 170 batch loss: 50.779213 batch xy loss 7.23706627 batch wh loss 11.1473551 batch obj loss 27.6457 batch_class_loss 4.74908876 epoch total loss: 17680.3848\n","Trained batch: 171 batch loss: 65.8682404 batch xy loss 8.79849243 batch wh loss 15.9303579 batch obj loss 35.4580307 batch_class_loss 5.68135786 epoch total loss: 17746.2539\n","Trained batch: 172 batch loss: 42.8758774 batch xy loss 5.52015877 batch wh loss 9.00931454 batch obj loss 24.2835712 batch_class_loss 4.06282949 epoch total loss: 17789.1289\n","Trained batch: 173 batch loss: 41.4799538 batch xy loss 5.98366 batch wh loss 6.96538687 batch obj loss 25.2809277 batch_class_loss 3.24997902 epoch total loss: 17830.6094\n","Trained batch: 174 batch loss: 45.0042114 batch xy loss 6.02238274 batch wh loss 8.95366287 batch obj loss 27.0722675 batch_class_loss 2.9558959 epoch total loss: 17875.6133\n","Trained batch: 175 batch loss: 51.939209 batch xy loss 9.15421486 batch wh loss 9.17630863 batch obj loss 31.5981216 batch_class_loss 2.01055813 epoch total loss: 17927.5527\n","Trained batch: 176 batch loss: 60.4248657 batch xy loss 8.58708477 batch wh loss 13.6970615 batch obj loss 31.4090157 batch_class_loss 6.73170137 epoch total loss: 17987.9785\n","Trained batch: 177 batch loss: 45.7030106 batch xy loss 5.89566898 batch wh loss 10.4042616 batch obj loss 25.3703918 batch_class_loss 4.03268337 epoch total loss: 18033.6816\n","Trained batch: 178 batch loss: 50.9163 batch xy loss 8.33377457 batch wh loss 10.5233202 batch obj loss 28.2108631 batch_class_loss 3.84834647 epoch total loss: 18084.5977\n","Trained batch: 179 batch loss: 41.566494 batch xy loss 6.57510138 batch wh loss 7.85285234 batch obj loss 25.2718239 batch_class_loss 1.86671495 epoch total loss: 18126.1641\n","Trained batch: 180 batch loss: 107.32885 batch xy loss 6.84427 batch wh loss 71.6614075 batch obj loss 23.8011036 batch_class_loss 5.02208 epoch total loss: 18233.4922\n","Trained batch: 181 batch loss: 51.3115273 batch xy loss 8.57171917 batch wh loss 8.9155941 batch obj loss 30.2276897 batch_class_loss 3.59651756 epoch total loss: 18284.8047\n","Trained batch: 182 batch loss: 52.3311157 batch xy loss 8.45032215 batch wh loss 9.59993362 batch obj loss 31.028656 batch_class_loss 3.25220919 epoch total loss: 18337.1367\n","Trained batch: 183 batch loss: 49.5975075 batch xy loss 8.63626099 batch wh loss 7.54316807 batch obj loss 30.2325344 batch_class_loss 3.18554521 epoch total loss: 18386.7344\n","Trained batch: 184 batch loss: 842.807129 batch xy loss 10.6895819 batch wh loss 793.54364 batch obj loss 35.9086914 batch_class_loss 2.66515207 epoch total loss: 19229.541\n","Trained batch: 185 batch loss: 48.8720589 batch xy loss 7.02629423 batch wh loss 8.55048847 batch obj loss 29.3454304 batch_class_loss 3.94984603 epoch total loss: 19278.4121\n","Trained batch: 186 batch loss: 50.1976814 batch xy loss 9.03193665 batch wh loss 8.25030518 batch obj loss 31.2547569 batch_class_loss 1.66068316 epoch total loss: 19328.6094\n","Trained batch: 187 batch loss: 53.1630783 batch xy loss 7.43314743 batch wh loss 11.7267399 batch obj loss 28.1931686 batch_class_loss 5.81002665 epoch total loss: 19381.7715\n","Trained batch: 188 batch loss: 36.8391151 batch xy loss 5.58463478 batch wh loss 8.75609779 batch obj loss 19.5212078 batch_class_loss 2.97717404 epoch total loss: 19418.6113\n","Trained batch: 189 batch loss: 53.0380096 batch xy loss 7.81494617 batch wh loss 12.5558348 batch obj loss 28.9372044 batch_class_loss 3.7300241 epoch total loss: 19471.6484\n","Trained batch: 190 batch loss: 83.6843643 batch xy loss 11.1110716 batch wh loss 23.2064381 batch obj loss 42.5574493 batch_class_loss 6.80940628 epoch total loss: 19555.332\n","Trained batch: 191 batch loss: 40.2376976 batch xy loss 5.9586668 batch wh loss 8.39493561 batch obj loss 23.6193542 batch_class_loss 2.26473713 epoch total loss: 19595.5703\n","Trained batch: 192 batch loss: 57.7763176 batch xy loss 9.68722916 batch wh loss 14.6262245 batch obj loss 30.4565468 batch_class_loss 3.00631952 epoch total loss: 19653.3457\n","Trained batch: 193 batch loss: 56.6265678 batch xy loss 8.67096329 batch wh loss 13.7971411 batch obj loss 29.4289513 batch_class_loss 4.72951698 epoch total loss: 19709.9727\n","Trained batch: 194 batch loss: 63.7115631 batch xy loss 9.25140285 batch wh loss 15.6516619 batch obj loss 31.2522144 batch_class_loss 7.55628681 epoch total loss: 19773.6836\n","Trained batch: 195 batch loss: 60.1058769 batch xy loss 8.05932236 batch wh loss 14.4799938 batch obj loss 30.9511032 batch_class_loss 6.61545801 epoch total loss: 19833.7891\n","Trained batch: 196 batch loss: 65.0199738 batch xy loss 9.66628551 batch wh loss 15.0927629 batch obj loss 35.3059845 batch_class_loss 4.95494127 epoch total loss: 19898.8086\n","Trained batch: 197 batch loss: 51.8425941 batch xy loss 7.40027046 batch wh loss 10.7587118 batch obj loss 30.7383118 batch_class_loss 2.94529939 epoch total loss: 19950.6504\n","Trained batch: 198 batch loss: 66.6208878 batch xy loss 10.6013184 batch wh loss 13.0755816 batch obj loss 36.1448059 batch_class_loss 6.79918337 epoch total loss: 20017.2715\n","Trained batch: 199 batch loss: 66.8276138 batch xy loss 10.1158609 batch wh loss 14.6520758 batch obj loss 37.3077087 batch_class_loss 4.75196409 epoch total loss: 20084.1\n","Trained batch: 200 batch loss: 57.5532 batch xy loss 7.80781174 batch wh loss 13.6996269 batch obj loss 31.7990875 batch_class_loss 4.24667358 epoch total loss: 20141.6523\n","Trained batch: 201 batch loss: 43.5374184 batch xy loss 6.59208488 batch wh loss 8.6795969 batch obj loss 24.7583523 batch_class_loss 3.50738263 epoch total loss: 20185.1895\n","Trained batch: 202 batch loss: 54.0780067 batch xy loss 7.68813801 batch wh loss 10.5660124 batch obj loss 31.6929111 batch_class_loss 4.13095 epoch total loss: 20239.2676\n","Trained batch: 203 batch loss: 70.3061829 batch xy loss 9.75883579 batch wh loss 15.7856026 batch obj loss 37.5808182 batch_class_loss 7.18091965 epoch total loss: 20309.5742\n","Trained batch: 204 batch loss: 107.821877 batch xy loss 7.51400852 batch wh loss 66.2756195 batch obj loss 30.9022312 batch_class_loss 3.1300168 epoch total loss: 20417.3965\n","Trained batch: 205 batch loss: 62.5948219 batch xy loss 7.6513896 batch wh loss 17.5194778 batch obj loss 29.7246227 batch_class_loss 7.69933319 epoch total loss: 20479.9922\n","Trained batch: 206 batch loss: 61.3527222 batch xy loss 9.99218369 batch wh loss 13.5227232 batch obj loss 34.7271156 batch_class_loss 3.11069274 epoch total loss: 20541.3457\n","Trained batch: 207 batch loss: 54.1357117 batch xy loss 8.21570587 batch wh loss 11.8297596 batch obj loss 31.7682247 batch_class_loss 2.32202148 epoch total loss: 20595.4805\n","Trained batch: 208 batch loss: 52.0192032 batch xy loss 7.39681053 batch wh loss 9.72627068 batch obj loss 29.7984982 batch_class_loss 5.09762 epoch total loss: 20647.5\n","Trained batch: 209 batch loss: 42.7576332 batch xy loss 6.40841 batch wh loss 7.46868229 batch obj loss 26.4311752 batch_class_loss 2.44936585 epoch total loss: 20690.2578\n","Trained batch: 210 batch loss: 65.2890778 batch xy loss 8.44481945 batch wh loss 16.8145103 batch obj loss 33.3658905 batch_class_loss 6.66385651 epoch total loss: 20755.5469\n","Trained batch: 211 batch loss: 60.9949417 batch xy loss 8.2546072 batch wh loss 14.044858 batch obj loss 33.5844841 batch_class_loss 5.11099815 epoch total loss: 20816.541\n","Trained batch: 212 batch loss: 63.8242 batch xy loss 9.69553757 batch wh loss 10.6850271 batch obj loss 38.6593208 batch_class_loss 4.78431416 epoch total loss: 20880.3652\n","Trained batch: 213 batch loss: 53.4611893 batch xy loss 8.29378319 batch wh loss 11.081213 batch obj loss 28.9790306 batch_class_loss 5.107162 epoch total loss: 20933.8262\n","Trained batch: 214 batch loss: 48.0054932 batch xy loss 7.91290522 batch wh loss 7.22899151 batch obj loss 29.2240944 batch_class_loss 3.63950396 epoch total loss: 20981.832\n","Trained batch: 215 batch loss: 65.1991272 batch xy loss 10.3755589 batch wh loss 13.2481737 batch obj loss 37.2306709 batch_class_loss 4.34472132 epoch total loss: 21047.0312\n","Trained batch: 216 batch loss: 55.6506844 batch xy loss 8.8659668 batch wh loss 9.74316788 batch obj loss 32.8956757 batch_class_loss 4.14587259 epoch total loss: 21102.6816\n","Trained batch: 217 batch loss: 48.6996536 batch xy loss 8.07787514 batch wh loss 7.77942657 batch obj loss 29.7185612 batch_class_loss 3.12379551 epoch total loss: 21151.3809\n","Trained batch: 218 batch loss: 55.0424271 batch xy loss 8.33246422 batch wh loss 11.1373491 batch obj loss 29.5926628 batch_class_loss 5.97995138 epoch total loss: 21206.4238\n","Trained batch: 219 batch loss: 30.3935642 batch xy loss 4.35821533 batch wh loss 5.38759613 batch obj loss 18.4498 batch_class_loss 2.19795322 epoch total loss: 21236.8184\n","Trained batch: 220 batch loss: 42.6434746 batch xy loss 5.6076088 batch wh loss 9.66186428 batch obj loss 26.0166397 batch_class_loss 1.35736144 epoch total loss: 21279.4609\n","Trained batch: 221 batch loss: 67.3860779 batch xy loss 9.71515846 batch wh loss 17.317831 batch obj loss 33.7378693 batch_class_loss 6.61521769 epoch total loss: 21346.8477\n","Trained batch: 222 batch loss: 52.2493477 batch xy loss 7.18511772 batch wh loss 11.7082033 batch obj loss 28.1362743 batch_class_loss 5.2197547 epoch total loss: 21399.0977\n","Trained batch: 223 batch loss: 68.3127747 batch xy loss 11.61936 batch wh loss 13.2769794 batch obj loss 38.0633392 batch_class_loss 5.3530941 epoch total loss: 21467.4102\n","Trained batch: 224 batch loss: 67.6776199 batch xy loss 9.46218777 batch wh loss 14.0250607 batch obj loss 40.0263977 batch_class_loss 4.16397238 epoch total loss: 21535.0879\n","Trained batch: 225 batch loss: 55.4505196 batch xy loss 7.5116744 batch wh loss 13.1009188 batch obj loss 27.5694752 batch_class_loss 7.2684474 epoch total loss: 21590.5391\n","Trained batch: 226 batch loss: 43.0577583 batch xy loss 6.3084569 batch wh loss 7.41839266 batch obj loss 26.8377552 batch_class_loss 2.49315214 epoch total loss: 21633.5977\n","Trained batch: 227 batch loss: 47.1940689 batch xy loss 5.87570238 batch wh loss 10.0269318 batch obj loss 24.4919491 batch_class_loss 6.79948664 epoch total loss: 21680.791\n","Trained batch: 228 batch loss: 66.9283524 batch xy loss 10.5279598 batch wh loss 15.5179825 batch obj loss 33.9925652 batch_class_loss 6.88984919 epoch total loss: 21747.7188\n","Trained batch: 229 batch loss: 67.7266 batch xy loss 8.01131439 batch wh loss 17.2501 batch obj loss 31.2924747 batch_class_loss 11.1727104 epoch total loss: 21815.4453\n","Trained batch: 230 batch loss: 53.9935532 batch xy loss 8.82228661 batch wh loss 10.3940182 batch obj loss 29.1617088 batch_class_loss 5.61553478 epoch total loss: 21869.4395\n","Trained batch: 231 batch loss: 69.1086197 batch xy loss 10.366806 batch wh loss 18.5505905 batch obj loss 33.3709869 batch_class_loss 6.82022667 epoch total loss: 21938.5488\n","Trained batch: 232 batch loss: 53.5604897 batch xy loss 7.80617189 batch wh loss 12.8165188 batch obj loss 28.6235428 batch_class_loss 4.31426 epoch total loss: 21992.1094\n","Trained batch: 233 batch loss: 76.6689072 batch xy loss 11.7603359 batch wh loss 19.7063408 batch obj loss 37.5075874 batch_class_loss 7.69463921 epoch total loss: 22068.7773\n","Trained batch: 234 batch loss: 69.150032 batch xy loss 9.19138145 batch wh loss 18.2258186 batch obj loss 34.5347672 batch_class_loss 7.1980629 epoch total loss: 22137.9277\n","Trained batch: 235 batch loss: 60.2735062 batch xy loss 8.33040619 batch wh loss 11.972827 batch obj loss 35.0398407 batch_class_loss 4.93043232 epoch total loss: 22198.2012\n","Trained batch: 236 batch loss: 46.4800148 batch xy loss 7.0411849 batch wh loss 8.62789059 batch obj loss 27.6374798 batch_class_loss 3.17345834 epoch total loss: 22244.6816\n","Trained batch: 237 batch loss: 47.6006927 batch xy loss 7.74507332 batch wh loss 9.94295883 batch obj loss 26.0455132 batch_class_loss 3.86714649 epoch total loss: 22292.2832\n","Trained batch: 238 batch loss: 45.2767067 batch xy loss 6.14129829 batch wh loss 8.46050167 batch obj loss 25.891489 batch_class_loss 4.78341818 epoch total loss: 22337.5605\n","Trained batch: 239 batch loss: 65.5402908 batch xy loss 9.07792759 batch wh loss 20.2472057 batch obj loss 32.2384644 batch_class_loss 3.97669625 epoch total loss: 22403.1016\n","Trained batch: 240 batch loss: 48.4142303 batch xy loss 7.59580374 batch wh loss 7.87893486 batch obj loss 29.1310959 batch_class_loss 3.80839372 epoch total loss: 22451.5156\n","Trained batch: 241 batch loss: 62.3254204 batch xy loss 9.72154617 batch wh loss 10.1584311 batch obj loss 36.6309357 batch_class_loss 5.81450701 epoch total loss: 22513.8418\n","Trained batch: 242 batch loss: 43.3708153 batch xy loss 7.48234844 batch wh loss 6.50415277 batch obj loss 26.0239334 batch_class_loss 3.36037374 epoch total loss: 22557.2129\n","Trained batch: 243 batch loss: 70.5808105 batch xy loss 10.0782928 batch wh loss 17.6533508 batch obj loss 36.6568108 batch_class_loss 6.19235945 epoch total loss: 22627.793\n","Trained batch: 244 batch loss: 65.4397812 batch xy loss 8.30240822 batch wh loss 19.6296387 batch obj loss 29.5114059 batch_class_loss 7.99633 epoch total loss: 22693.2324\n","Trained batch: 245 batch loss: 44.7093849 batch xy loss 7.51695585 batch wh loss 7.93725491 batch obj loss 26.3939857 batch_class_loss 2.86118698 epoch total loss: 22737.9414\n","Trained batch: 246 batch loss: 48.1905441 batch xy loss 7.95428276 batch wh loss 9.19735 batch obj loss 26.2747192 batch_class_loss 4.76419783 epoch total loss: 22786.1328\n","Trained batch: 247 batch loss: 72.113266 batch xy loss 8.89257717 batch wh loss 26.4375267 batch obj loss 31.1385708 batch_class_loss 5.64459372 epoch total loss: 22858.2461\n","Trained batch: 248 batch loss: 46.4690552 batch xy loss 8.07441425 batch wh loss 7.74911118 batch obj loss 28.2893791 batch_class_loss 2.35614681 epoch total loss: 22904.7148\n","Trained batch: 249 batch loss: 361.324707 batch xy loss 7.29008675 batch wh loss 320.801025 batch obj loss 28.7697639 batch_class_loss 4.46381855 epoch total loss: 23266.0391\n","Trained batch: 250 batch loss: 49.8722954 batch xy loss 6.14058113 batch wh loss 10.192482 batch obj loss 28.6748 batch_class_loss 4.86443233 epoch total loss: 23315.9121\n","Trained batch: 251 batch loss: 38.3581467 batch xy loss 6.26533461 batch wh loss 5.90620089 batch obj loss 24.9582348 batch_class_loss 1.22837543 epoch total loss: 23354.2695\n","Trained batch: 252 batch loss: 51.8753853 batch xy loss 6.7104454 batch wh loss 10.5489712 batch obj loss 27.3899822 batch_class_loss 7.22598553 epoch total loss: 23406.1445\n","Trained batch: 253 batch loss: 37.215641 batch xy loss 5.46316767 batch wh loss 7.42416048 batch obj loss 21.3104496 batch_class_loss 3.01786256 epoch total loss: 23443.3594\n","Trained batch: 254 batch loss: 65.0287247 batch xy loss 9.82480812 batch wh loss 14.1337862 batch obj loss 34.9401169 batch_class_loss 6.13001156 epoch total loss: 23508.3887\n","Trained batch: 255 batch loss: 61.6579399 batch xy loss 8.14938927 batch wh loss 14.5812235 batch obj loss 32.7732315 batch_class_loss 6.15409136 epoch total loss: 23570.0469\n","Trained batch: 256 batch loss: 36.3231964 batch xy loss 5.45930481 batch wh loss 6.67049265 batch obj loss 22.4501781 batch_class_loss 1.74321854 epoch total loss: 23606.3691\n","Trained batch: 257 batch loss: 66.4801559 batch xy loss 10.733551 batch wh loss 13.0518885 batch obj loss 38.3995 batch_class_loss 4.29522514 epoch total loss: 23672.8496\n","Trained batch: 258 batch loss: 41.3329277 batch xy loss 6.78645039 batch wh loss 6.6314497 batch obj loss 25.6139946 batch_class_loss 2.30103397 epoch total loss: 23714.1816\n","Trained batch: 259 batch loss: 51.3780022 batch xy loss 9.12204 batch wh loss 8.9370842 batch obj loss 29.1276741 batch_class_loss 4.19120598 epoch total loss: 23765.5605\n","Trained batch: 260 batch loss: 72.0456238 batch xy loss 11.4118967 batch wh loss 16.4313889 batch obj loss 37.0740623 batch_class_loss 7.12827492 epoch total loss: 23837.6055\n","Trained batch: 261 batch loss: 50.742981 batch xy loss 7.18283606 batch wh loss 9.90392113 batch obj loss 28.3600044 batch_class_loss 5.2962203 epoch total loss: 23888.3477\n","Trained batch: 262 batch loss: 55.8545 batch xy loss 9.32625 batch wh loss 10.9688749 batch obj loss 30.6308365 batch_class_loss 4.92853832 epoch total loss: 23944.2031\n","Trained batch: 263 batch loss: 40.6607208 batch xy loss 5.96043348 batch wh loss 6.70761538 batch obj loss 24.3460464 batch_class_loss 3.64662528 epoch total loss: 23984.8633\n","Trained batch: 264 batch loss: 66.8623886 batch xy loss 9.98761368 batch wh loss 14.8067579 batch obj loss 35.8759 batch_class_loss 6.19211817 epoch total loss: 24051.7266\n","Trained batch: 265 batch loss: 59.2304878 batch xy loss 10.4099407 batch wh loss 11.3029642 batch obj loss 33.133564 batch_class_loss 4.38401842 epoch total loss: 24110.957\n","Trained batch: 266 batch loss: 48.4347496 batch xy loss 7.96109819 batch wh loss 8.11431885 batch obj loss 28.82901 batch_class_loss 3.53032351 epoch total loss: 24159.3926\n","Trained batch: 267 batch loss: 65.7973633 batch xy loss 9.73528385 batch wh loss 15.1898918 batch obj loss 33.8127632 batch_class_loss 7.05942822 epoch total loss: 24225.1895\n","Trained batch: 268 batch loss: 39.3784637 batch xy loss 5.39875221 batch wh loss 6.69299841 batch obj loss 23.7080231 batch_class_loss 3.57868242 epoch total loss: 24264.5684\n","Trained batch: 269 batch loss: 47.7414513 batch xy loss 7.16071 batch wh loss 8.42412281 batch obj loss 28.3410988 batch_class_loss 3.815521 epoch total loss: 24312.3105\n","Trained batch: 270 batch loss: 42.7154083 batch xy loss 7.13520718 batch wh loss 7.67394114 batch obj loss 24.8836212 batch_class_loss 3.02263856 epoch total loss: 24355.0254\n","Trained batch: 271 batch loss: 55.9171 batch xy loss 6.93337202 batch wh loss 14.1151752 batch obj loss 28.9844589 batch_class_loss 5.88408852 epoch total loss: 24410.9434\n","Trained batch: 272 batch loss: 43.105854 batch xy loss 7.77966261 batch wh loss 7.59147596 batch obj loss 25.0088749 batch_class_loss 2.72583652 epoch total loss: 24454.0488\n","Trained batch: 273 batch loss: 49.1648521 batch xy loss 8.32430458 batch wh loss 8.6807 batch obj loss 28.6090965 batch_class_loss 3.55075049 epoch total loss: 24503.2129\n","Trained batch: 274 batch loss: 50.2718964 batch xy loss 8.14242458 batch wh loss 11.0594482 batch obj loss 26.8687057 batch_class_loss 4.20132256 epoch total loss: 24553.4844\n","Trained batch: 275 batch loss: 52.8093071 batch xy loss 7.64106226 batch wh loss 12.5467367 batch obj loss 28.0328197 batch_class_loss 4.58869267 epoch total loss: 24606.293\n","Trained batch: 276 batch loss: 52.5772362 batch xy loss 7.63424921 batch wh loss 8.34831 batch obj loss 30.7431259 batch_class_loss 5.85155 epoch total loss: 24658.8711\n","Trained batch: 277 batch loss: 37.9917 batch xy loss 5.3766284 batch wh loss 7.33881474 batch obj loss 22.1315918 batch_class_loss 3.14466286 epoch total loss: 24696.8633\n","Trained batch: 278 batch loss: 46.2494316 batch xy loss 7.80877209 batch wh loss 8.36878681 batch obj loss 25.7660198 batch_class_loss 4.30585241 epoch total loss: 24743.1133\n","Trained batch: 279 batch loss: 49.8817711 batch xy loss 6.41185474 batch wh loss 12.3343878 batch obj loss 26.2690258 batch_class_loss 4.86650038 epoch total loss: 24792.9941\n","Trained batch: 280 batch loss: 47.2329979 batch xy loss 6.88683891 batch wh loss 10.5255079 batch obj loss 26.095871 batch_class_loss 3.72477913 epoch total loss: 24840.2266\n","Trained batch: 281 batch loss: 55.4616241 batch xy loss 8.6771965 batch wh loss 12.1075382 batch obj loss 30.939949 batch_class_loss 3.73694158 epoch total loss: 24895.6875\n","Trained batch: 282 batch loss: 43.7362213 batch xy loss 6.95222521 batch wh loss 6.75114346 batch obj loss 26.6532364 batch_class_loss 3.37961674 epoch total loss: 24939.4238\n","Trained batch: 283 batch loss: 60.1997299 batch xy loss 8.91000938 batch wh loss 13.2506065 batch obj loss 31.7363567 batch_class_loss 6.30275583 epoch total loss: 24999.623\n","Trained batch: 284 batch loss: 49.8051758 batch xy loss 7.48807335 batch wh loss 9.59662914 batch obj loss 28.4919624 batch_class_loss 4.22851181 epoch total loss: 25049.4277\n","Trained batch: 285 batch loss: 57.9025459 batch xy loss 9.0515461 batch wh loss 11.9539804 batch obj loss 32.7736359 batch_class_loss 4.12338305 epoch total loss: 25107.3301\n","Trained batch: 286 batch loss: 54.5592079 batch xy loss 8.64049149 batch wh loss 11.5975828 batch obj loss 28.8038578 batch_class_loss 5.51728296 epoch total loss: 25161.8887\n","Trained batch: 287 batch loss: 44.7211075 batch xy loss 5.88973713 batch wh loss 8.53618431 batch obj loss 26.8840065 batch_class_loss 3.41118193 epoch total loss: 25206.6094\n","Trained batch: 288 batch loss: 37.7865295 batch xy loss 5.13411188 batch wh loss 7.18297052 batch obj loss 22.8684196 batch_class_loss 2.60103106 epoch total loss: 25244.3965\n","Trained batch: 289 batch loss: 38.146759 batch xy loss 6.90771818 batch wh loss 5.46753931 batch obj loss 23.3639412 batch_class_loss 2.40755844 epoch total loss: 25282.543\n","Trained batch: 290 batch loss: 64.3820724 batch xy loss 9.36604786 batch wh loss 16.467083 batch obj loss 32.400856 batch_class_loss 6.14808035 epoch total loss: 25346.9258\n","Trained batch: 291 batch loss: 40.6459579 batch xy loss 8.14991856 batch wh loss 4.28008 batch obj loss 25.9375076 batch_class_loss 2.27845359 epoch total loss: 25387.5723\n","Trained batch: 292 batch loss: 45.627636 batch xy loss 6.67181063 batch wh loss 9.65116 batch obj loss 24.8810635 batch_class_loss 4.42360258 epoch total loss: 25433.2\n","Trained batch: 293 batch loss: 55.0853806 batch xy loss 9.27776241 batch wh loss 8.64735699 batch obj loss 32.6628647 batch_class_loss 4.49739361 epoch total loss: 25488.2852\n","Trained batch: 294 batch loss: 49.7706642 batch xy loss 7.91226768 batch wh loss 9.16927052 batch obj loss 28.173914 batch_class_loss 4.51521301 epoch total loss: 25538.0566\n","Trained batch: 295 batch loss: 37.3948364 batch xy loss 5.18706036 batch wh loss 6.68439531 batch obj loss 22.8263741 batch_class_loss 2.69700646 epoch total loss: 25575.4512\n","Trained batch: 296 batch loss: 45.2893105 batch xy loss 6.0470376 batch wh loss 9.57072449 batch obj loss 25.9177132 batch_class_loss 3.75383472 epoch total loss: 25620.7402\n","Trained batch: 297 batch loss: 53.2116852 batch xy loss 8.27068233 batch wh loss 11.3065462 batch obj loss 29.8813477 batch_class_loss 3.75311327 epoch total loss: 25673.9512\n","Trained batch: 298 batch loss: 38.5933189 batch xy loss 5.91067743 batch wh loss 7.86627769 batch obj loss 21.1678314 batch_class_loss 3.64853144 epoch total loss: 25712.5449\n","Trained batch: 299 batch loss: 45.752594 batch xy loss 7.80265045 batch wh loss 7.38312531 batch obj loss 27.3229027 batch_class_loss 3.24391198 epoch total loss: 25758.2969\n","Trained batch: 300 batch loss: 52.8312836 batch xy loss 8.95290852 batch wh loss 9.0413456 batch obj loss 31.2606792 batch_class_loss 3.57634735 epoch total loss: 25811.1289\n","Trained batch: 301 batch loss: 50.459053 batch xy loss 9.41655 batch wh loss 9.64579582 batch obj loss 28.062664 batch_class_loss 3.33404708 epoch total loss: 25861.5879\n","Trained batch: 302 batch loss: 48.381134 batch xy loss 6.61952209 batch wh loss 9.24215794 batch obj loss 28.4814434 batch_class_loss 4.03801155 epoch total loss: 25909.9688\n","Trained batch: 303 batch loss: 47.6652031 batch xy loss 7.77905226 batch wh loss 9.4573679 batch obj loss 26.5669975 batch_class_loss 3.86178517 epoch total loss: 25957.6348\n","Trained batch: 304 batch loss: 47.5449066 batch xy loss 8.76108646 batch wh loss 6.15246201 batch obj loss 29.3540688 batch_class_loss 3.27728915 epoch total loss: 26005.1797\n","Trained batch: 305 batch loss: 49.8706818 batch xy loss 8.82850266 batch wh loss 9.81318188 batch obj loss 26.9031372 batch_class_loss 4.32586479 epoch total loss: 26055.0508\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mYe-SQufu_0D","colab_type":"text"},"source":["**Validation**\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"XvWnCDVGuAV-","colab_type":"code","colab":{}},"source":["model = YoloV3(shape=(416, 416, 5), num_classes=num_classes, training=False)\n","model.load_weights('/content/gdrive/My Drive/kitti/models/model-v1.0.1-epoch-10-loss-20.3276.tf')\n","postprocess = Postprocessor(iou_thresh=0.5, score_thresh=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0T1Zt_CT_mcV","colab_type":"code","colab":{}},"source":["files_list=['/content/gdrive/My Drive/kitti/tfrecords/val3.tfrecords',\n","            '/content/gdrive/My Drive/kitti/tfrecords/val7.tfrecords',\n","            '/content/gdrive/My Drive/kitti/tfrecords/val10.tfrecords',\n","            '/content/gdrive/My Drive/kitti/tfrecords/val22.tfrecords',\n","            '/content/gdrive/My Drive/kitti/tfrecords/val26.tfrecords',\n","            '/content/gdrive/My Drive/kitti/tfrecords/val35.tfrecords',\n","            '/content/gdrive/My Drive/kitti/tfrecords/val44.tfrecords',\n","            '/content/gdrive/My Drive/kitti/tfrecords/val53.tfrecords',\n","            '/content/gdrive/My Drive/kitti/tfrecords/val62.tfrecords',\n","            '/content/gdrive/My Drive/kitti/tfrecords/val65.tfrecords',\n","            '/content/gdrive/My Drive/kitti/tfrecords/val71.tfrecords',\n","            '/content/gdrive/My Drive/kitti/tfrecords/val85.tfrecords',\n","            '/content/gdrive/My Drive/kitti/tfrecords/val91.tfrecords',\n","            '/content/gdrive/My Drive/kitti/tfrecords/val98.tfrecords']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tMXv8NpIA0pN","colab_type":"code","colab":{}},"source":["dims_list = ['/content/gdrive/My Drive/kitti/input/val_img_dims/val_image_dims0.csv',\n","             '/content/gdrive/My Drive/kitti/input/val_img_dims/val_image_dims1.csv',\n","             '/content/gdrive/My Drive/kitti/input/val_img_dims/val_image_dims2.csv',\n","             '/content/gdrive/My Drive/kitti/input/val_img_dims/val_image_dims3.csv',\n","             '/content/gdrive/My Drive/kitti/input/val_img_dims/val_image_dims4.csv',\n","             '/content/gdrive/My Drive/kitti/input/val_img_dims/val_image_dims5.csv',\n","             '/content/gdrive/My Drive/kitti/input/val_img_dims/val_image_dims6.csv',\n","             '/content/gdrive/My Drive/kitti/input/val_img_dims/val_image_dims7.csv',\n","             '/content/gdrive/My Drive/kitti/input/val_img_dims/val_image_dims8.csv',\n","             '/content/gdrive/My Drive/kitti/input/val_img_dims/val_image_dims9.csv',\n","             '/content/gdrive/My Drive/kitti/input/val_img_dims/val_image_dims10.csv',\n","             '/content/gdrive/My Drive/kitti/input/val_img_dims/val_image_dims11.csv',\n","             '/content/gdrive/My Drive/kitti/input/val_img_dims/val_image_dims12.csv',\n","             '/content/gdrive/My Drive/kitti/input/val_img_dims/val_image_dims13.csv'] "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MV1Vi0EqyhW0","colab_type":"code","colab":{}},"source":["for tf_file, df_file in zip (files_list, dims_list):\n","  preprocess = Preprocessor('combined')\n","  dataset = tf.data.Dataset.list_files(tf_file)\n","  dataset = tf.data.TFRecordDataset(dataset)\n","  dataset = dataset.map(preprocess)\n","  df = pd.read_csv(df_file)\n","  for data,n in zip (dataset, range(len(df))):\n","    filename = str(df.loc[n]['filename'])\n","    w = df.loc[n]['width']\n","    h = df.loc[n]['height']\n","    df1 = pd.DataFrame(columns= ['class_name','Probability','xmin','ymin','xmax','ymax'])\n","    x,y_true = data\n","    x = tf.expand_dims(x,axis=0)\n","    y_pred = model(x,training =False)\n","    y_pred_nms = postprocess(y_pred)\n","\n","    num_predictions = y_pred_nms[3][0][0].numpy()\n","    predictions.append(num_predictions)\n","\n","    for i in range(0,num_predictions):\n","      box = y_pred_nms[0][0][i].numpy()\n","      score = y_pred_nms[1][0][i][0].numpy()\n","      class_prob = y_pred_nms[2][0][i].numpy()\n","      clob_prob = class_prob * score\n","      values, indices = tf.math.top_k(clob_prob, k=1)\n","      for index, prob in zip(indices.numpy(), values.numpy()):\n","        confidence = prob\n","        class_name = class_names[index]\n","      xmin = box[0]* w\n","      ymin = box[1]* h\n","      xmax = box[2]* w \n","      ymax = box[3]* h\n","      df1.loc[len(df1)] = [class_name,confidence,xmin,ymin,xmax,ymax]\n","    #result.append(df1)\n","    df1.to_csv(dest_path+'/'+filename, header=None, index=None, sep=' ', mode='a')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jK-Xi5glMbAp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594728569448,"user_tz":-330,"elapsed":384968,"user":{"displayName":"Gokulesh Danapal","photoUrl":"","userId":"12285818100145340354"}},"outputId":"b26ba8d6-bcba-41fb-e7e9-fb5fa41b1448"},"source":["!python main.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":[": cannot connect to X server \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RQyWVhwI5wmC","colab_type":"text"},"source":["Show result on image"]},{"cell_type":"code","metadata":{"id":"Gz_Xp-zhysiQ","colab_type":"code","colab":{}},"source":["class_names= ['Car','Cyclist','Pedestrian']\n","colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']\n","x1 = x[:,:,:,0:3]\n","im = tf.cast((x1[0] + 1) * 127.5, tf.int32)\n","\n","plt.rcParams['figure.figsize'] = (20,20)\n","fig,ax = plt.subplots(1)\n","ax.imshow(im)\n","h, w, d = im.shape\n","\n","for i in range(0, num_predictions):\n","    box = y_pred_nms[0][0][i].numpy()\n","    score = y_pred_nms[1][0][i][0].numpy()\n","    class_prob = y_pred_nms[2][0][i].numpy()\n","    clob_prob = class_prob * score\n","    values, indices = tf.math.top_k(clob_prob, k=1)\n","    text= []\n","    for index, prob in zip(indices.numpy(), values.numpy()):\n","        text.append('{} {:.1f}%'.format(class_names[index], prob*100))\n","    text = ', '.join(text)\n","    xmin = box[0] * w\n","    ymin = box[1] * h\n","    width = (box[2] - box[0]) * w \n","    height = (box[3] - box[1]) * h\n","    color = colors[i % 8]\n","    rect = patches.Rectangle((xmin,ymin),width,height,linewidth=2,edgecolor=color,facecolor='none')\n","    ax.add_patch(rect)\n","    ax.annotate(text, (xmin, ymin - 5), color=color, weight='bold', \n","                fontsize=18)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"22QoSr5y33OE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594727964665,"user_tz":-330,"elapsed":947,"user":{"displayName":"Gokulesh Danapal","photoUrl":"","userId":"12285818100145340354"}},"outputId":"c7fbfafb-47c0-4dc5-dd56-4332d3871682"},"source":["ituy = class_prob * score\n","ituy*100"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([21.150715  ,  0.0921298 ,  0.25406817], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":118}]},{"cell_type":"markdown","metadata":{"id":"qejW53IUvb7K","colab_type":"text"},"source":["Test for bugs"]},{"cell_type":"code","metadata":{"id":"T5z10stbmTmB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594594849629,"user_tz":-330,"elapsed":2135,"user":{"displayName":"Gokulesh Danapal","photoUrl":"","userId":"12285818100145340354"}},"outputId":"1f708d39-37fb-4041-a90e-2d5f9e483c27"},"source":["postprocess = Postprocessor(iou_thresh=0.5, score_thresh=0.0)\n","output=[]\n","for tensor in y_true:\n","  output.append(tf.split(tensor,[4,1,3],axis=-1))\n","y_true_1 = (output[0],output[1],output[2])\n","y_true_nms = postprocess(y_true_1)\n","\n","num_predictions = y_true_nms[3][0][0].numpy()\n","print('Number of predictions: ', num_predictions)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of predictions:  100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"W1kJ3rjM2vYZ","colab":{}},"source":["class_names=['Car','Cyclist','Pedestrian']\n","im = tf.reshape(x,[416,416,3])\n","colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']\n","#im = tf.cast((x[0] + 1) * 127.5, tf.int32)\n","\n","plt.rcParams['figure.figsize'] = (20,20)\n","fig,ax = plt.subplots(1)\n","ax.imshow(im)\n","h, w, d = im.shape\n","\n","for i in range(0, num_predictions):\n","    box = y_true_nms[0][0][i].numpy()\n","    score = y_true_nms[1][0][i][0].numpy()\n","    class_prob = y_true_nms[2][0][i].numpy()\n","    values, indices = tf.math.top_k(class_prob, k=3)\n","    text= []\n","    for index, prob in zip(indices.numpy(), values.numpy()):\n","        text.append('{} {:.1f}%'.format(class_names[index], prob*100))\n","    text = ', '.join(text)\n","    xmin = box[0] * w\n","    ymin = box[1] * h\n","    width = (box[2] - box[0]) * w\n","    height = (box[3] - box[1]) * h\n","    color = colors[i % 8]\n","    rect = patches.Rectangle((xmin,ymin),width,height,linewidth=2,edgecolor=color,facecolor='None')\n","    ax.add_patch(rect)\n","    #ax.annotate(text, (xmin, ymin - 5), color=color, weight='bold', \n","                #fontsize=18)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"adNdCIMjOHSy","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}